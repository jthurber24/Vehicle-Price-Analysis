# -*- coding: utf-8 -*-
"""Final Project Code

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-qrbbBLMh1Fhsw-36jDvH9AgqIk-tWuz
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

data = pd.read_csv('car_prices.csv')
print(data)

"""Data Cleaning"""

clean1 = data.dropna()
clean1.reset_index()
clean1 = clean1[:100000]
clean1

sns.scatterplot(clean1, x="odometer", y="sellingprice", s=30)
plt.xlabel("Odometer (km)")
plt.ylabel("Selling Price")
plt.title("Selling Price vs. Odometer Reading")
plt.grid(True)

plt.show()

sns.scatterplot(clean1, x="mmr", y="sellingprice", s=30)
plt.xlabel("MMR")
plt.ylabel("Selling Price")
plt.title("Selling Price vs. MMR")
plt.grid(True)

plt.show()

sns.relplot(
    x="condition",
    y="sellingprice",
    color="#E38804",
    kind="line",
    data=clean1,
    linewidth=2,
    marker="o",
    markersize=8,
    alpha=0.7,
    dashes=False,
    legend="full",
)


plt.xlabel("Condition")
plt.ylabel("Selling Price")
plt.title("Selling Price vs. Condition")


plt.show()

values = clean1["make"].value_counts().sort_values(ascending=False)
val = values.values
model = values.index

sns.barplot(
    x=model[0:10],
    y=val[0:10],
    errwidth=2,
    capsize=7,
)

plt.xlabel("Car Model (Top 10)")
plt.ylabel("Value")
plt.title("Distribution of Value for Top 10 Car Models")
plt.xticks(rotation=45, ha="right")
plt.tight_layout()

plt.show()

year_df = clean1.groupby(by="year", as_index=False)["sellingprice"].first()

fig = sns.barplot(year_df, x="year", y="sellingprice")
plt.xticks(rotation=90)
plt.show()

data_by_seller = clean1.groupby(by=["seller", "year"], as_index=False)[
    "sellingprice"
].first()
data_by_seller.sort_values(by="sellingprice", ascending=False, inplace=True)

sns.barplot(
    data_by_seller,
    x="year",
    y="sellingprice",
)
plt.title("Selling Price by Year")
plt.xlabel("Year")
plt.ylabel("Selling Price")
plt.xticks(rotation=90)
plt.show()

max_price_index = data_by_seller["sellingprice"].idxmax()
row_with_max_price = data_by_seller.loc[max_price_index]

seller_with_max_price = row_with_max_price["seller"]
max_selling_price = row_with_max_price["sellingprice"]
year_of_max_price = row_with_max_price["year"]

print(
    f"The seller, {seller_with_max_price}, bought at the maximum selling price of {max_selling_price:.1f} in year {year_of_max_price}."
)

make_price = clean1.groupby(by="make", as_index=False)["sellingprice"].first()
make_price.sort_values(by="sellingprice", ascending=False, inplace=True)

make_price.head(10)

max_price_make = make_price.head(1)["make"].values[0]

min_price_make = make_price.tail(1)["make"].values[0]

print("Car make with the maximum selling price:", max_price_make)
print("Car make with the minimum selling price:", min_price_make)

fig, axs = plt.subplots(1, 2, figsize=(8, 5))


sns.barplot(data=make_price[0:5], x="make", y="sellingprice", ax=axs[0])
axs[0].set_title("Top 5 Make vs Selling Price")
axs[0].set_xlabel("Make")
axs[0].set_ylabel("Selling Price")
axs[0].set_xticklabels(axs[0].get_xticklabels(), rotation=45, ha="right")


sns.barplot(data=make_price[-6:-1], x="make", y="sellingprice", ax=axs[1])
axs[1].set_title("Bottom 5 Make vs Selling Price")
axs[1].set_xlabel("Make")
axs[1].set_ylabel("Selling Price")
axs[1].set_xticklabels(axs[1].get_xticklabels(), rotation=45, ha="right")


plt.tight_layout()


plt.show()

new_df = clean1.groupby(
    by=["year", "make", "transmission", "condition", "color", "odometer", "mmr"],
    as_index=False,
)["sellingprice"].first()


new_df.sort_values(by="sellingprice", ascending=False).head(2)

yearly_mean_price = (
    new_df.groupby("year", as_index=False)["sellingprice"].mean()
).round(2)

sns.barplot(
    x=yearly_mean_price["year"],
    y=yearly_mean_price["sellingprice"]
)

plt.title("Mean Selling Price by Year")
plt.xlabel("Year")
plt.ylabel("Mean Selling Price")
plt.xticks(rotation=90)

plt.show()

filtered_df = clean1[clean1["color"] != "—"]

palette = "Set3"

plt.figure(figsize=(9, 5))
sns.barplot(
    filtered_df,
    x="color",
    y="sellingprice"
)
plt.xlabel("Color Category")
plt.ylabel("Selling Price")
plt.title("Selling Price by Color")

plt.xticks(rotation=90)

plt.show()

top_makes_per_year = new_df.groupby("year", as_index=False).apply(
    lambda x: x.nlargest(1, "sellingprice", keep="all")
)

top_makes_per_year = top_makes_per_year.rename(
    columns={"sellingprice": "highest_mean_price"}
)

sns.barplot(top_makes_per_year, x="year", y="highest_mean_price")

plt.xticks(rotation=90)
plt.show()

g = sns.FacetGrid(top_makes_per_year, col="make", col_wrap=3, aspect=2)

g.map(sns.barplot, "year", "highest_mean_price", linewidth=1, alpha=0.8)

g.fig.suptitle("Highest Mean Selling Price by Year (Faceted by Make)")
plt.tight_layout()
plt.show()

import numpy as np

# Convert quantitive datatypes to numerics
datatypes = {'quant': ['year', 'odometer', 'sellingprice'],
             'qual': ['make', 'transmission', 'model', 'trim', 'body', 'condition', 'state', 'color', 'interior']}
quants = clean1[datatypes['quant']].astype(np.float_)
clean2 = pd.concat([quants, clean1[datatypes['qual']]], axis=1)
sns.pairplot(clean1);

# Select only numeric columns for correlation calculation
numeric_cols = clean1.select_dtypes(include=[np.number])
corr_matrix = numeric_cols.corr().abs()
# Plotting
f, ax = plt.subplots(figsize=(12, 9))
sns.heatmap(corr_matrix, vmax=1, square=True, annot=True, fmt='.2f', cmap='coolwarm')
plt.xticks(rotation=90)
plt.yticks(rotation=0)
plt.show()

standardization_map = {
    'vw': 'Volkswagen',
    'ford tk': 'Ford',
    'airstream': 'Airstream',
    'chev truck': 'Chevrolet',
    'dot' : 'Dodge',
    'dodge tk': 'Dodge',
    'ford tk': 'Ford',
    'ford truck': 'Ford',
    'gmc truck': 'GMC',
    'hyundai tk': 'Hyundai',
    'landrover': 'Land Rover',
    'mazda tk': 'Mazda',
    'mercedes-b': 'Mercedes-Benz',
    'mercedes': 'Mercedes-Benz',
    'smart': 'Smart',
}

clean1['make'] = clean1['make'].replace(standardization_map, regex=False)

standardization_map = {
    '3-sep': '9-3',
    '5-sep' : '9-5',
    '3': 'Mazda3',
    '6': 'Mazda6'
}

clean1['model'] = clean1['model'].replace(standardization_map)

unique_models = clean1['model'].unique()

mask = ~clean1['color'].str.contains('â€”') & ~clean1['interior'].str.contains('â€”')

clean2 = clean1[mask]

clean2 = clean2[(clean2['sellingprice'] > 150) & (clean2['sellingprice'] <= 190000)]

clean2 = clean2[clean2['condition'] >= 11]
clean2

#one hot encoding
df_encoded = pd.get_dummies(clean2, columns=['make', 'transmission', 'body', 'state', 'color', 'interior'], dtype='int')
df_encoded.columns

withMMR = df_encoded.drop(columns=['vin', 'saledate', 'trim', 'seller', 'model'])
noMMR = df_encoded.drop(columns=['vin', 'saledate', 'mmr', 'trim', 'seller', 'model'])

noMMR.reset_index()

withMMR.describe()

"""# Visualizing the features"""

import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

"""Qualitative Variables"""

qualitative_counts = clean2['make'].value_counts()
print(qualitative_counts)

# Plot the counts on a bar chart
qualitative_counts.plot(kind='bar')

# Add labels and title
plt.xlabel('Categories')
plt.ylabel('Counts')
plt.title('Counts of Qualitative Data')

# Show the plot
plt.show()

"""Scatter Matrix"""

# Convert quantitive datatypes to numerics
datatypes = {'quant': ['year', 'odometer', 'sellingprice'],
             'qual': ['make', 'transmission', 'model', 'trim', 'body', 'condition', 'state', 'color', 'interior']}
quants = clean2[datatypes['quant']].astype(np.float_)
clean2 = pd.concat([quants, clean2[datatypes['qual']]], axis=1)
sns.pairplot(clean2);

# Select only numeric columns for correlation calculation
numeric_cols = clean2.select_dtypes(include=[np.number])
corr_matrix = numeric_cols.corr().abs()
# Plotting
f, ax = plt.subplots(figsize=(12, 9))
sns.heatmap(corr_matrix, vmax=1, square=True, annot=True, fmt='.2f', cmap='coolwarm')
plt.xticks(rotation=90)
plt.yticks(rotation=0)
plt.show()

"""#**Models**"""

from sklearn.model_selection import KFold
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error

"""### KFold Split"""

# Define the KFold object
kfold = KFold(n_splits=5, shuffle=True, random_state=42)

# Create empty lists for training and testing data
X_train_list = []
y_train_list = []
X_test_list = []
y_test_list = []

# Iterate through each fold
for train_index, test_index in kfold.split(withMMR):
    # Get the training and testing data for this fold
    X_train, X_test = withMMR.iloc[train_index], withMMR.iloc[test_index]

    # Get the target variables
    y_train, y_test = X_train["sellingprice"], X_test["sellingprice"]

    # Remove 'sellingprice' column from feature variables
    X_train = X_train.drop(columns=['sellingprice'])
    X_test = X_test.drop(columns=['sellingprice'])



    # Append the data to the lists
    X_train_list.append(X_train)
    y_train_list.append(y_train)
    X_test_list.append(X_test)
    y_test_list.append(y_test)

"""# Linear Regression"""

# Initialize Linear Regression model
lr_regressor = LinearRegression()

# Train and evaluate the model for each fold
for i in range(5):
    # Fit the model on training data
    lr_regressor.fit(X_train_list[i], y_train_list[i])

    # Predict on testing data
    y_pred = lr_regressor.predict(X_test_list[i])

    # Calculate Mean Squared Error
    mse = mean_squared_error(y_test_list[i], y_pred)

    # Print the MSE for each fold
    print(f"Fold {i+1}: Mean Squared Error = {mse}")

# Predict on the entire testing data
y_pred = lr_regressor.predict(X_test)

# Calculate evaluation metrics on the entire testing data
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)

# Print evaluation metrics on the entire testing data
print("Mean Square Error:", mse)
print("R Squared:", r2)
print("Mean Absolute Error:", mae)

len(lr_regressor.coef_)

"""# Decision Tree"""

# Initialize Decision Tree Regressor model
dt_regressor = DecisionTreeRegressor()

# Train and evaluate the model for each fold
for i in range(5):
    # Fit the model on training data
    dt_regressor.fit(X_train_list[i], y_train_list[i])

    # Predict on testing data
    y_pred = dt_regressor.predict(X_test_list[i])

    # Calculate Mean Squared Error
    mse = mean_squared_error(y_test_list[i], y_pred)

    # Print the MSE for each fold
    print(f"Fold {i+1}: Mean Squared Error = {mse}")

# Predict on the entire testing data
y_pred = dt_regressor.predict(X_test)

# Calculate evaluation metrics on the entire testing data
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)

# Print evaluation metrics on the entire testing data
print("Mean Square Error:", mse)
print("R Squared:", r2)
print("Mean Absolute Error:", mae)

"""# Random Forest"""

rf_regressor = RandomForestRegressor(n_estimators=100, max_depth=17, random_state=42)
for i in range(5):
    rf_regressor.fit(X_train_list[i], y_train_list[i])
    y_pred = rf_regressor.predict(X_test_list[i])
    mse = mean_squared_error(y_test_list[i], y_pred)
    print(f"Fold {i+1}: Mean Squared Error = {mse}")

y_pred = rf_regressor.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print("Mean Square Error:", mse)

r2 = r2_score(y_test, y_pred)
print("R Squared:", r2)

mae = mean_absolute_error(y_test, y_pred)
print("Mean Absolute Error:", mae)

from google.colab import drive
drive.mount('/content/drive')

"""# KNN"""

# Initialize KNN regressor model
knn_regressor = KNeighborsRegressor(n_neighbors=20)

# Train and evaluate the model for each fold
for i in range(5):
    # Fit the model on training data
    knn_regressor.fit(X_train_list[i], y_train_list[i])

    # Predict on testing data
    y_pred = knn_regressor.predict(X_test_list[i])

    # Calculate Mean Squared Error
    mse = mean_squared_error(y_test_list[i], y_pred)

    # Print the MSE for each fold
    print(f"Fold {i+1}: Mean Squared Error = {mse}")

# Predict on the entire testing data
y_pred = knn_regressor.predict(X_test)

# Calculate evaluation metrics on the entire testing data
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)

# Print evaluation metrics on the entire testing data
print("Mean Square Error:", mse)
print("R Squared:", r2)
print("Mean Absolute Error:", mae)

!pip install catboost

from catboost import CatBoostRegressor

# Initialize CatBoostRegressor model
catboost_regressor = CatBoostRegressor(iterations=1500, depth=8, learning_rate=0.1, loss_function='RMSE', random_state=42)

# Train and evaluate the model for each fold
for i in range(5):
    # Fit the model on training data
    catboost_regressor.fit(X_train_list[i], y_train_list[i], eval_set=(X_test_list[i], y_test_list[i]), verbose=100)

    # Predict on testing data
    y_pred = catboost_regressor.predict(X_test_list[i])

    # Calculate Mean Squared Error
    mse = mean_squared_error(y_test_list[i], y_pred)

    # Print the MSE for each fold
    print(f"Fold {i+1}: Mean Squared Error = {mse}")

# Predict on the entire testing data
y_pred = catboost_regressor.predict(X_test)

# Calculate evaluation metrics on the entire testing data
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)

# Print evaluation metrics on the entire testing data
print("Mean Square Error:", mse)
print("R Squared:", r2)
print("Mean Absolute Error:", mae)

"""#**Models with MMR**

### KFold Split
"""

# Define the KFold object
kfold = KFold(n_splits=5, shuffle=True, random_state=42)

# Create empty lists for training and testing data
X_train_list = []
y_train_list = []
X_test_list = []
y_test_list = []

# Iterate through each fold
for train_index, test_index in kfold.split(withMMR):
    # Get the training and testing data for this fold
    X_train, X_test = withMMR.iloc[train_index], withMMR.iloc[test_index]

    # Get the target variables
    y_train, y_test = X_train["sellingprice"], X_test["sellingprice"]

    # Remove 'sellingprice' column from feature variables
    X_train = X_train.drop(columns=['sellingprice'])
    X_test = X_test.drop(columns=['sellingprice'])



    # Append the data to the lists
    X_train_list.append(X_train)
    y_train_list.append(y_train)
    X_test_list.append(X_test)
    y_test_list.append(y_test)

"""# Random Forest"""

rf_regressor = RandomForestRegressor(n_estimators=100, max_depth=17, random_state=42)
for i in range(5):
    rf_regressor.fit(X_train_list[i], y_train_list[i])
    y_pred = rf_regressor.predict(X_test_list[i])
    mse = mean_squared_error(y_test_list[i], y_pred)
    print(f"Fold {i+1}: Mean Squared Error = {mse}")

y_pred = rf_regressor.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print("Mean Square Error:", mse)

r2 = r2_score(y_test, y_pred)
print("R Squared:", r2)

mae = mean_absolute_error(y_test, y_pred)
print("Mean Absolute Error:", mae)

from catboost import CatBoostRegressor

# Initialize CatBoostRegressor model
catboost_regressor = CatBoostRegressor(iterations=1500, depth=6, learning_rate=0.1, loss_function='RMSE', random_state=42)

# Train and evaluate the model for each fold
for i in range(5):
    # Fit the model on training data
    catboost_regressor.fit(X_train_list[i], y_train_list[i], eval_set=(X_test_list[i], y_test_list[i]), verbose=100)

    # Predict on testing data
    y_pred = catboost_regressor.predict(X_test_list[i])

    # Calculate Mean Squared Error
    mse = mean_squared_error(y_test_list[i], y_pred)

    # Print the MSE for each fold
    print(f"Fold {i+1}: Mean Squared Error = {mse}")

# Predict on the entire testing data
y_pred = catboost_regressor.predict(X_test)

# Calculate evaluation metrics on the entire testing data
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)

# Print evaluation metrics on the entire testing data
print("Mean Square Error:", mse)
print("R Squared:", r2)
print("Mean Absolute Error:", mae)